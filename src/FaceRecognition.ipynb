{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataLoader import DataLoader\n",
    "from utils.modelLoader import ModelLoader\n",
    "from utils.SiameseModel import SiameseModel\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs Triplet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {\n",
    "    \"Siamese_MobileNetV1_Train500_All\": \"metadate_all_limit500.json\",\n",
    "    \"Siamese_MobileNetV1_Train5000_All\": \"metadate_all_limit5000.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 21s 1s/step - loss: 15.1831 - val_loss: 9.7778\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 17s 762ms/step - loss: 7.4801 - val_loss: 11.7879\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 7s 651ms/step - loss: 6.2252 - val_loss: 1.2277\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 8s 701ms/step - loss: 1.6107 - val_loss: 0.4155\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 8s 704ms/step - loss: 1.3626 - val_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 7s 619ms/step - loss: 0.9180 - val_loss: 0.3723\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 7s 652ms/step - loss: 0.8589 - val_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 7s 628ms/step - loss: 0.8390 - val_loss: 0.0884\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 7s 637ms/step - loss: 0.2744 - val_loss: 0.1495\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 7s 655ms/step - loss: 2.9813 - val_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 7s 663ms/step - loss: 0.9335 - val_loss: 0.0000e+00\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 7s 632ms/step - loss: 0.8446 - val_loss: 1.4096\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 7s 602ms/step - loss: 0.2823 - val_loss: 0.0000e+00\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 7s 623ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 7s 640ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 7s 631ms/step - loss: 0.0029 - val_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 7s 594ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 7s 658ms/step - loss: 0.1282 - val_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 7s 650ms/step - loss: 0.2254 - val_loss: 0.0000e+00\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 7s 669ms/step - loss: 0.0975 - val_loss: 0.0000e+00\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train500_All\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train500_All\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train500_All_embedding\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train500_All_embedding\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 294s 2s/step - loss: 9.4159 - val_loss: 3.0401\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 192s 2s/step - loss: 2.4327 - val_loss: 0.7893\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 39s 290ms/step - loss: 0.9454 - val_loss: 0.3586\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 40s 297ms/step - loss: 0.4501 - val_loss: 0.2368\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 40s 295ms/step - loss: 0.3195 - val_loss: 0.2313\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 41s 300ms/step - loss: 0.2635 - val_loss: 0.2507\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 41s 303ms/step - loss: 0.2607 - val_loss: 0.2307\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 43s 315ms/step - loss: 0.1826 - val_loss: 0.1565\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 44s 321ms/step - loss: 0.2087 - val_loss: 0.1895\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 47s 336ms/step - loss: 0.2650 - val_loss: 0.2438\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 49s 346ms/step - loss: 0.2153 - val_loss: 0.1149\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 51s 362ms/step - loss: 0.2287 - val_loss: 0.2038\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 55s 389ms/step - loss: 0.2823 - val_loss: 0.4264\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 59s 417ms/step - loss: 0.2980 - val_loss: 1.2923\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 59s 403ms/step - loss: 0.4575 - val_loss: 1.9235\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 63s 435ms/step - loss: 1.2676 - val_loss: 0.9487\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 65s 444ms/step - loss: 1.1185 - val_loss: 1.0422\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 69s 470ms/step - loss: 1.3504 - val_loss: 1.4661\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 70s 477ms/step - loss: 0.9031 - val_loss: 0.7101\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 73s 492ms/step - loss: 0.6856 - val_loss: 0.1238\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train5000_All\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train5000_All\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train5000_All_embedding\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Train5000_All_embedding\\assets\n"
     ]
    }
   ],
   "source": [
    "for run in runs.keys():\n",
    "    train_ds, val_ds, test_ds = DataLoader().loadDatasets(\"img_align_celeba\", runs[run], 32)\n",
    "    \n",
    "    model, embedding = ModelLoader().loadMobileNetV1FaceRecognition(True)\n",
    "    siamese_model = SiameseModel(siamese_network=model)\n",
    "    siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), weighted_metrics=[])\n",
    "    \n",
    "    log_dir = \"../logs/fit/\" + run\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    siamese_model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[tensorboard_callback])\n",
    "    \n",
    "    runPath = \"../models/\" + run\n",
    "    model.save(runPath)\n",
    "    embedding.save(runPath + \"_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = DataLoader().loadDatasets(\"img_align_celeba\", \"metadate_identity-once_limit500.json\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save(\"../models/Siamese_MobileNetV1_Train500\")\n",
    "embedding.save(\"../models/Siamese_MobileNetV1_Train500_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "embedding = tf.keras.models.load_model(\"../models/Siamese_MobileNetV1_Train5000_All_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive similarity: 0.9566349\n",
      "Negative similarity 0.919959\n",
      "Difference:                         0.03667587\n"
     ]
    }
   ],
   "source": [
    "for sample in iter(test_ds):\n",
    "    anchor, positive, negative = sample\n",
    "\n",
    "    anchor_embedding, positive_embedding, negative_embedding = (\n",
    "        embedding(tf.keras.applications.mobilenet.preprocess_input(anchor)),\n",
    "        embedding(tf.keras.applications.mobilenet.preprocess_input(positive)),\n",
    "        embedding(tf.keras.applications.mobilenet.preprocess_input(negative)),\n",
    "    )\n",
    "    \n",
    "    cosine_similarity = tf.keras.metrics.CosineSimilarity()\n",
    "\n",
    "    positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "    print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "    negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "    print(\"Negative similarity\", negative_similarity.numpy())\n",
    "    \n",
    "    print(\"Difference:                        \", positive_similarity.numpy()-negative_similarity.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y, preds, margin=1):\n",
    "    y = tf.cast(y, preds.dtype)\n",
    "    squaredPreds = tf.keras.backend.square(preds)\n",
    "    squaredMargin = tf.keras.backend.square(tf.keras.backend.maximum(margin - preds, 0))\n",
    "    loss = tf.keras.backend.mean(y * squaredPreds + (1 - y) * squaredMargin) \n",
    "    return 1-loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pair data cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "478/478 [==============================] - 1211s 2s/step - loss: 0.5058 - accuracy: 0.4977 - val_loss: 0.4999 - val_accuracy: 0.5005\n",
      "Epoch 2/20\n",
      "478/478 [==============================] - 1088s 2s/step - loss: 0.5004 - accuracy: 0.4996 - val_loss: 0.4852 - val_accuracy: 0.5148\n",
      "Epoch 3/20\n",
      "478/478 [==============================] - 1037s 2s/step - loss: 0.5033 - accuracy: 0.4968 - val_loss: 0.4954 - val_accuracy: 0.5047\n",
      "Epoch 4/20\n",
      "478/478 [==============================] - 908s 2s/step - loss: 0.4989 - accuracy: 0.5011 - val_loss: 0.4985 - val_accuracy: 0.5017\n",
      "Epoch 5/20\n",
      "478/478 [==============================] - 928s 2s/step - loss: 0.5006 - accuracy: 0.4994 - val_loss: 0.4953 - val_accuracy: 0.5047\n",
      "Epoch 6/20\n",
      "478/478 [==============================] - 923s 2s/step - loss: 0.4980 - accuracy: 0.5020 - val_loss: 0.4999 - val_accuracy: 0.5002\n",
      "Epoch 7/20\n",
      "478/478 [==============================] - 931s 2s/step - loss: 0.5019 - accuracy: 0.4981 - val_loss: 0.5023 - val_accuracy: 0.4977\n",
      "Epoch 8/20\n",
      "478/478 [==============================] - 902s 2s/step - loss: 0.5051 - accuracy: 0.4949 - val_loss: 0.4996 - val_accuracy: 0.5005\n",
      "Epoch 9/20\n",
      "478/478 [==============================] - 683s 1s/step - loss: 0.4977 - accuracy: 0.5023 - val_loss: 0.4938 - val_accuracy: 0.5063\n",
      "Epoch 10/20\n",
      "478/478 [==============================] - 755s 1s/step - loss: 0.4974 - accuracy: 0.5026 - val_loss: 0.4999 - val_accuracy: 0.5002\n",
      "Epoch 11/20\n",
      "478/478 [==============================] - 738s 1s/step - loss: 0.5004 - accuracy: 0.4996 - val_loss: 0.5050 - val_accuracy: 0.4950\n",
      "Epoch 12/20\n",
      "478/478 [==============================] - 901s 2s/step - loss: 0.5011 - accuracy: 0.4989 - val_loss: 0.5054 - val_accuracy: 0.4947\n",
      "Epoch 13/20\n",
      "478/478 [==============================] - 837s 1s/step - loss: 0.5021 - accuracy: 0.4979 - val_loss: 0.5053 - val_accuracy: 0.4947\n",
      "Epoch 14/20\n",
      "478/478 [==============================] - 847s 1s/step - loss: 0.5025 - accuracy: 0.4975 - val_loss: 0.4940 - val_accuracy: 0.5060\n",
      "Epoch 15/20\n",
      "478/478 [==============================] - 864s 1s/step - loss: 0.5000 - accuracy: 0.5000 - val_loss: 0.4889 - val_accuracy: 0.5112\n",
      "Epoch 16/20\n",
      "478/478 [==============================] - 835s 1s/step - loss: 0.4976 - accuracy: 0.5024 - val_loss: 0.5130 - val_accuracy: 0.4870\n",
      "Epoch 17/20\n",
      "478/478 [==============================] - 914s 1s/step - loss: 0.4996 - accuracy: 0.5004 - val_loss: 0.4999 - val_accuracy: 0.5002\n",
      "Epoch 18/20\n",
      "478/478 [==============================] - 905s 1s/step - loss: 0.4972 - accuracy: 0.5028 - val_loss: 0.5136 - val_accuracy: 0.4864\n",
      "Epoch 19/20\n",
      "478/478 [==============================] - 898s 1s/step - loss: 0.5010 - accuracy: 0.4990 - val_loss: 0.5044 - val_accuracy: 0.4956\n",
      "Epoch 20/20\n",
      "478/478 [==============================] - 943s 2s/step - loss: 0.5032 - accuracy: 0.4968 - val_loss: 0.5081 - val_accuracy: 0.4919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Pairs_Train200p_All_E20\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Pairs_Train200p_All_E20\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Pairs_Train200_All_E20_Embedding\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/Siamese_MobileNetV1_Pairs_Train200_All_E20_Embedding\\assets\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = DataLoader().loadDatasetsPairs(\"img_align_celeba\", \"metadate_pairs_all_limit500.json\", 32, False)\n",
    "\n",
    "model, embedding = ModelLoader().loadMobileNetV1FaceRecognitionPair()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=contrastive_loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "log_dir = \"../logs/fit/Siamese_MobileNetV1_Pairs_Train500_All_E20\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=[tensorboard_callback])\n",
    "\n",
    "model.save(\"../models/Siamese_MobileNetV1_Pairs_Train200p_All_E20\")\n",
    "embedding.save(\"../models/Siamese_MobileNetV1_Pairs_Train200_All_E20_Embedding\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "478/478 [==============================] - 1266s 2s/step - loss: 0.4988 - accuracy: 0.5023 - val_loss: 0.5005 - val_accuracy: 0.4995\n",
      "Epoch 2/20\n",
      "478/478 [==============================] - 1251s 2s/step - loss: 0.4997 - accuracy: 0.5004 - val_loss: 0.5148 - val_accuracy: 0.4852\n",
      "Epoch 3/20\n",
      "478/478 [==============================] - 1289s 2s/step - loss: 0.4968 - accuracy: 0.5032 - val_loss: 0.5047 - val_accuracy: 0.4953\n",
      "Epoch 4/20\n",
      "478/478 [==============================] - 1325s 2s/step - loss: 0.5011 - accuracy: 0.4989 - val_loss: 0.5017 - val_accuracy: 0.4983\n",
      "Epoch 5/20\n",
      "478/478 [==============================] - 1328s 2s/step - loss: 0.4994 - accuracy: 0.5006 - val_loss: 0.5047 - val_accuracy: 0.4953\n",
      "Epoch 6/20\n",
      "478/478 [==============================] - 1341s 2s/step - loss: 0.5020 - accuracy: 0.4980 - val_loss: 0.5002 - val_accuracy: 0.4998\n",
      "Epoch 7/20\n",
      "478/478 [==============================] - 1386s 2s/step - loss: 0.4981 - accuracy: 0.5019 - val_loss: 0.4977 - val_accuracy: 0.5023\n",
      "Epoch 8/20\n",
      "478/478 [==============================] - 1444s 2s/step - loss: 0.4949 - accuracy: 0.5051 - val_loss: 0.5005 - val_accuracy: 0.4995\n",
      "Epoch 9/20\n",
      "478/478 [==============================] - 1400s 2s/step - loss: 0.5023 - accuracy: 0.4977 - val_loss: 0.5063 - val_accuracy: 0.4937\n",
      "Epoch 10/20\n",
      "478/478 [==============================] - 1410s 2s/step - loss: 0.5026 - accuracy: 0.4974 - val_loss: 0.5002 - val_accuracy: 0.4998\n",
      "Epoch 11/20\n",
      "478/478 [==============================] - 1396s 2s/step - loss: 0.4996 - accuracy: 0.5004 - val_loss: 0.4950 - val_accuracy: 0.5050\n",
      "Epoch 12/20\n",
      "478/478 [==============================] - 1412s 2s/step - loss: 0.4989 - accuracy: 0.5011 - val_loss: 0.4946 - val_accuracy: 0.5053\n",
      "Epoch 13/20\n",
      "478/478 [==============================] - 1405s 2s/step - loss: 0.4979 - accuracy: 0.5021 - val_loss: 0.4947 - val_accuracy: 0.5053\n",
      "Epoch 14/20\n",
      "478/478 [==============================] - 1243s 2s/step - loss: 0.4975 - accuracy: 0.5025 - val_loss: 0.5060 - val_accuracy: 0.4940\n",
      "Epoch 15/20\n",
      "478/478 [==============================] - 1254s 2s/step - loss: 0.5000 - accuracy: 0.5000 - val_loss: 0.5112 - val_accuracy: 0.4888\n",
      "Epoch 16/20\n",
      "478/478 [==============================] - 1301s 2s/step - loss: 0.5024 - accuracy: 0.4976 - val_loss: 0.4870 - val_accuracy: 0.5130\n",
      "Epoch 17/20\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = DataLoader().loadDatasetsPairs(\"img_align_celeba\", \"metadate_pairs_all_limit500.json\", 32)\n",
    "\n",
    "model, embedding = ModelLoader().loadMobileNetV1FaceRecognitionPair()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=contrastive_loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "log_dir = \"../logs/fit/Siamese_MobileNetV1_Pairs_Train500_Crop_All_E20\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=[tensorboard_callback])\n",
    "\n",
    "model.save(\"../models/Siamese_MobileNetV1_Pairs_Train200_Crop_All_E20\")\n",
    "embedding.save(\"../models/Siamese_MobileNetV1_Pairs_Train200_Crop_All_E20_Embedding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1650262b3ee0ad320e518d32138bb4c67705e5f1b5fd0593bdd8b873d187d5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
